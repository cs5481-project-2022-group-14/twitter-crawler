{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25d0954e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import tarfile\n",
    "import gzip\n",
    "import os\n",
    "import nltk \n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize   \n",
    "from nltk.corpus import stopwords         \n",
    "from nltk.stem import WordNetLemmatizer   \n",
    "from nltk.stem import PorterStemmer      \n",
    "import string\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "885f3fbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 4864/4864 [01:16<00:00, 63.31it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 242/242 [00:03<00:00, 63.06it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 6567/6567 [02:25<00:00, 45.29it/s]\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "for root,dirs,files in os.walk(\"./tweets\"):\n",
    "    for i in tqdm(range(len(files))):\n",
    "        file = files[i]\n",
    "        file_path = os.path.join(root,file)\n",
    "        with open(file_path,'r',encoding='UTF-8') as csvfile:\n",
    "            csv_reader = csv.reader(csvfile) \n",
    "            header = next(csv_reader)  \n",
    "            for row in csv_reader:  \n",
    "                if len(row)==3:\n",
    "                    id = row[0]\n",
    "                    created_at = row[1]\n",
    "                    full_text = row[2]\n",
    "                    with open(\"full_text.csv\",'a',newline=\"\", encoding= 'UTF-8') as csv_file:\n",
    "                        csvwriter = csv.writer(csv_file)\n",
    "                        csvwriter.writerow([id, created_at,full_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "841a2559",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_text = pd.read_csv(\"data_processed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cd4139f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text_processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1587956423287508992</td>\n",
       "      <td>china beijing report new local symptomatic cor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1587956409366466560</td>\n",
       "      <td>mean use chinese app e share personal data chi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1587956402483503104</td>\n",
       "      <td>hat important thing important china china ukra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1587956384473260034</td>\n",
       "      <td>never flourish would inside worlds large marke...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1587956376931774465</td>\n",
       "      <td>seem uk policy make circle still explore way p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1136256</th>\n",
       "      <td>1591265116338548736</td>\n",
       "      <td>would constitution block republican meanwhile ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1136257</th>\n",
       "      <td>1591265106717192192</td>\n",
       "      <td>hate troll see russia aggrieve ukraine call wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1136258</th>\n",
       "      <td>1591265105358245888</td>\n",
       "      <td>ukraine crowdfunding new fleet drone boat take...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1136259</th>\n",
       "      <td>1591265101746929664</td>\n",
       "      <td>would think celebrate retreat russia ohh wait ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1136260</th>\n",
       "      <td>1591265093219921920</td>\n",
       "      <td>father bandera mom ukraine guess grandpa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1136261 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          id  \\\n",
       "0        1587956423287508992   \n",
       "1        1587956409366466560   \n",
       "2        1587956402483503104   \n",
       "3        1587956384473260034   \n",
       "4        1587956376931774465   \n",
       "...                      ...   \n",
       "1136256  1591265116338548736   \n",
       "1136257  1591265106717192192   \n",
       "1136258  1591265105358245888   \n",
       "1136259  1591265101746929664   \n",
       "1136260  1591265093219921920   \n",
       "\n",
       "                                            text_processed  \n",
       "0        china beijing report new local symptomatic cor...  \n",
       "1        mean use chinese app e share personal data chi...  \n",
       "2        hat important thing important china china ukra...  \n",
       "3        never flourish would inside worlds large marke...  \n",
       "4        seem uk policy make circle still explore way p...  \n",
       "...                                                    ...  \n",
       "1136256  would constitution block republican meanwhile ...  \n",
       "1136257  hate troll see russia aggrieve ukraine call wa...  \n",
       "1136258  ukraine crowdfunding new fleet drone boat take...  \n",
       "1136259  would think celebrate retreat russia ohh wait ...  \n",
       "1136260           father bandera mom ukraine guess grandpa  \n",
       "\n",
       "[1136261 rows x 2 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "aefeb47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_doc = dict(zip(data['id'],data[\"text_processed\"]))\n",
    "id_index = dict(zip(data['id'],data.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fab797c",
   "metadata": {},
   "source": [
    "## inverted index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "42012ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "invert_dict={}\n",
    "for i in range(len(data)):\n",
    "    id = id_text['id'][i]\n",
    "    text = id_text['text_processed'][i]\n",
    "    if type(text)!=str:\n",
    "        text = ''\n",
    "    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    for word in tokens:\n",
    "        if word in invert_dict:\n",
    "            invert_dict[word].append(id)\n",
    "        else:\n",
    "            invert_dict[word]=[id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f16c922e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#write csv\n",
    "import csv\n",
    "with open(\"index.csv\",'w',newline=\"\", encoding= 'UTF-8') as csv_file:\n",
    "    csvwriter = csv.writer(csv_file)\n",
    "    csvwriter.writerow([\"word\",\"ID\"])\n",
    "    for key in invert_dict:\n",
    "        csvwriter.writerow([key,invert_dict[key]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "39edef84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read csv\n",
    "import csv\n",
    "invert_dict={}\n",
    "index=pd.read_csv(\"index.csv\",header=None,encoding = \"UTF-8\")\n",
    "for i in range(1,len(index[0])):\n",
    "    w=[]\n",
    "    l=index[1][i][1:-1].split(',')\n",
    "    invert_dict[index[0][i]]=l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143de889",
   "metadata": {},
   "source": [
    "# Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ae8baf1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import gzip\n",
    "import os\n",
    "import nltk \n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize   \n",
    "from nltk.corpus import stopwords         \n",
    "from nltk.stem import WordNetLemmatizer   \n",
    "from nltk.stem import PorterStemmer      \n",
    "import string\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "import re\n",
    "def text_process(text):\n",
    "    res = []\n",
    "    if text:\n",
    "        text = text.strip('RT')\n",
    "        at_pattern = re.compile(\"@[\\S]*\")\n",
    "        url_pattern = re.compile(\"https:\\/\\/[\\w./]*[\\w/]\")\n",
    "        text = re.sub(at_pattern,'',text)\n",
    "        text = re.sub(url_pattern,'',text)\n",
    "        tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "        tokens = tokenizer.tokenize(text.lower())\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        stop_words.update(list(string.punctuation))\n",
    "        filtered = [w for w in tokens if w not in stop_words]\n",
    "        filtered = [w for w in tokens if not w.isdigit()]\n",
    "        lemmatizaer = WordNetLemmatizer()\n",
    "        for word, tag in nltk.pos_tag(filtered):\n",
    "            if tag.startswith('NN'):\n",
    "                word= lemmatizaer.lemmatize(word, pos='n')\n",
    "            elif tag.startswith('VB'):\n",
    "                word= lemmatizaer.lemmatize(word, pos='v')\n",
    "            elif tag.startswith('JJ'):\n",
    "                word= lemmatizaer.lemmatize(word, pos='a')\n",
    "            elif tag.startswith('R'):\n",
    "                word= lemmatizaer.lemmatize(word, pos='r')\n",
    "            res.append(word)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f9f7f4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"CHINA BEIJING CORONAVIRUS CASES 2022\"\n",
    "q = text_process(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "12751d3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['china', 'beijing', 'coronavirus', 'case']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2dc663a",
   "metadata": {},
   "source": [
    "# Boolean Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ea9884be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def boolean_query(q):\n",
    "    index=set(invert_dict[q[0]])\n",
    "    for word in q:\n",
    "        if(len(word)>1):\n",
    "            index=index&set(invert_dict[word])\n",
    "    index=list(index)\n",
    "    index.sort()\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5b097e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = boolean_query(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5bc40435",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1587956423287508992,\n",
       " 1588142773349371906,\n",
       " 1589406732773920768,\n",
       " 1589406754231779330,\n",
       " 1589407497634422785,\n",
       " 1589421735958429696,\n",
       " 1589812052046270466,\n",
       " 1589908442042404866,\n",
       " 1589909473841364995,\n",
       " 1590131480541687808,\n",
       " 1590131502179766272,\n",
       " 1590131506797719553,\n",
       " 1590131566923386881,\n",
       " 1590389848300728321,\n",
       " 1590492823107010560,\n",
       " 1590492844615155715,\n",
       " 1590492846137626625,\n",
       " 1590492905285771267,\n",
       " 1590494088255741953,\n",
       " 1590601401385697280]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e38517be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "china beijing report new local symptomatic coronavirus case asymptomatic case nov\n",
      "china covid universal resort shuts due beijing coronavirus case business\n",
      "china beijing report new local symptomatic coronavirus case asymptomatic case nov\n",
      "china beijing report new local symptomatic coronavirus case asymptomatic case nov\n",
      "china beijing report new local symptomatic coronavirus case asymptomatic case nov\n",
      "firstsquawk china beijing report new local symptomatic coronavirus case asymptomatic case nov\n",
      "nstworld china capital beijing report symptomatic asymptomatic case compare symptomatic asymptomatic case previous day local government data show china coronavirus infection\n",
      "china covid epicentre shift guangzhou outbreaks widen beijing november new coronavirus case surge guangzhou chinese city official data show tuesday global manufacturing hub become\n",
      "beijing new coronavirus case surge guangzhou chinese city official data show tuesday global manufacturing hub become china late epicentre test city ability avoid shanghai style lockdown\n",
      "china beijing report new local symptomatic coronavirus case asymptomatic case nov\n",
      "china beijing report new local symptomatic coronavirus case asymptomatic case nov\n",
      "china beijing report new local symptomatic coronavirus case asymptomatic case nov\n",
      "china beijing report new local symptomatic coronavirus case asymptomatic case nov ainvest onlinetrading usstock tradingtips view\n",
      "andy vermaut share covid case surge xinjiang despite strict lockdown measure public demonstration continue beijing china november ani public outrage china xinjiang region continue coronavirus case surge thank\n",
      "china beijing report new local symptomatic coronavirus case asymptomatic case nov\n",
      "china beijing report new local symptomatic coronavirus case asymptomatic case nov\n",
      "china beijing report new local symptomatic coronavirus case asymptomatic case nov\n",
      "china beijing report new local symptomatic coronavirus case asymptomatic case nov ainvest market nasdaq tradingtips view\n",
      "beijing report new coronavirus case low global standard high china capital\n",
      "beijing report new coronavirus case low global standard high china capital\n"
     ]
    }
   ],
   "source": [
    "for doc in index:\n",
    "    print(doc_dict[doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c67036",
   "metadata": {},
   "source": [
    "# Sort by TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "46d0c0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer  \n",
    "from sklearn.feature_extraction.text import TfidfTransformer \n",
    "corpus = id_text['text_processed'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e4f3a7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(corpus)):\n",
    "    if type(corpus[i])!=str:\n",
    "        corpus[i]=''\n",
    "    corpus[i].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1c5b8914",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer  \n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import operator\n",
    "\n",
    "## Create Vocabulary\n",
    "vectorizer = CountVectorizer()  \n",
    "X = vectorizer.fit_transform(corpus)  \n",
    "vocabulary = vectorizer.get_feature_names_out() \n",
    "# Intializating the tfIdf model\n",
    "tfidf = TfidfVectorizer(vocabulary=vocabulary)\n",
    "# Fit the TfIdf model\n",
    "tfidf.fit(corpus)\n",
    "# Transform the TfIdf model\n",
    "tfidf_tran=tfidf.transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1fbb2fd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1136261, 208629)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_tran.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ec75317e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tfidf_tran)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "638b1bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "sparse.save_npz('tf_idf.npz', tfidf_tran)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62d7424",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = sparse.load_npz('tf_idf.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "99dc43e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_vector(tokens):\n",
    "    Q = np.zeros((len(vocabulary)))   \n",
    "    for token in tokens:\n",
    "        try:\n",
    "            ind = tfidf.vocabulary_[token]\n",
    "            Q[ind]  += 1/len(tokens)\n",
    "        except:\n",
    "            pass     \n",
    "    for token in tokens:\n",
    "        try:\n",
    "            ind = tfidf.vocabulary_[token]\n",
    "            Q[ind]  = tfidf.idf_[ind]\n",
    "        except:\n",
    "            pass\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a2b8141f",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_vector = gen_vector(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e6a78758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6107428986720824, 0.5356085059414503, 0.6107428986720824, 0.6107428986720824, 0.6107428986720824, 0.5406374664321026, 0.3725669053914238, 0.3402297410808934, 0.34580807625424526, 0.6107428986720824, 0.6107428986720824, 0.6107428986720824, 0.43649124679078805, 0.3615591548329079, 0.6107428986720824, 0.6107428986720824, 0.6107428986720824, 0.47116954449854526, 0.6042208667041988, 0.6042208667041988]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy import spatial\n",
    "    \n",
    "#rank the documents in index\n",
    "d_cosines = []\n",
    "for doc in index:\n",
    "    #get the tfidf of doc\n",
    "    d = np.asarray(tfidf_tran.getrow(id_index[int(doc)]).todense())\n",
    "    #compute cosine similarity\n",
    "    cos_sim = cosine_similarity(query_vector.reshape(1, -1), d.reshape(1, -1))[0][0]\n",
    "    d_cosines.append(cos_sim)   \n",
    "out = np.array(d_cosines).argsort()[-10:][::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5a1e301b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1590131480541687808\n",
      "china beijing report new local symptomatic coronavirus case asymptomatic case nov\n",
      "1590131506797719553\n",
      "china beijing report new local symptomatic coronavirus case asymptomatic case nov\n",
      "1589406732773920768\n",
      "china beijing report new local symptomatic coronavirus case asymptomatic case nov\n",
      "1589406754231779330\n",
      "china beijing report new local symptomatic coronavirus case asymptomatic case nov\n",
      "1589407497634422785\n",
      "china beijing report new local symptomatic coronavirus case asymptomatic case nov\n",
      "1590131502179766272\n",
      "china beijing report new local symptomatic coronavirus case asymptomatic case nov\n",
      "1587956423287508992\n",
      "china beijing report new local symptomatic coronavirus case asymptomatic case nov\n",
      "1590492823107010560\n",
      "china beijing report new local symptomatic coronavirus case asymptomatic case nov\n",
      "1590492844615155715\n",
      "china beijing report new local symptomatic coronavirus case asymptomatic case nov\n",
      "1590492846137626625\n",
      "china beijing report new local symptomatic coronavirus case asymptomatic case nov\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(index[out[i]])\n",
    "    print(id_doc[index[out[i]]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd37f1c",
   "metadata": {},
   "source": [
    "# search engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efc1f2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import os\n",
    "import nltk \n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize   \n",
    "from nltk.corpus import stopwords         \n",
    "from nltk.stem import WordNetLemmatizer   \n",
    "from nltk.stem import PorterStemmer      \n",
    "import string\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer  \n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import operator\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy import spatial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03fafe7",
   "metadata": {},
   "source": [
    "load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "651b7a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_text = pd.read_csv(\"full_text.csv\",names=['ID', 'created_at','full_text'])\n",
    "id_text = pd.read_csv(\"data_processed.csv\")\n",
    "id_doc = dict(zip(id_text['id'],id_text[\"text_processed\"]))\n",
    "id_index = dict(zip(id_text['id'],id_text.index))\n",
    "invert_dict={}\n",
    "index_csv=pd.read_csv(\"index.csv\",header=None,encoding = \"UTF-8\")\n",
    "for i in range(1,len(index_csv[0])):\n",
    "    w=[]\n",
    "    l=index_csv[1][i][1:-1].split(',')\n",
    "    invert_dict[index_csv[0][i]]=l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac4b505",
   "metadata": {},
   "source": [
    "calculate tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ac81894",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1330: UserWarning: Upper case characters found in vocabulary while 'lowercase' is True. These entries will not be matched with any documents\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "corpus = id_text['text_processed'].tolist()\n",
    "for i in range(len(corpus)):\n",
    "    if type(corpus[i])!=str:\n",
    "        corpus[i]=''\n",
    "    corpus[i].lower()\n",
    "\n",
    "## Create Vocabulary\n",
    "vectorizer = CountVectorizer()  \n",
    "X = vectorizer.fit_transform(corpus)  \n",
    "vocabulary = vectorizer.get_feature_names_out() \n",
    "# Intializating the tfIdf model\n",
    "tfidf = TfidfVectorizer(vocabulary=vocabulary, lowercase = True)\n",
    "# Fit the TfIdf model\n",
    "tfidf.fit(corpus)\n",
    "# Transform the TfIdf model\n",
    "tfidf_tran=tfidf.transform(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ac42ce",
   "metadata": {},
   "source": [
    "search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb951ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_process(text):\n",
    "    res = []\n",
    "    if text:\n",
    "        text = text.strip('RT')\n",
    "        at_pattern = re.compile(\"@[\\S]*\")\n",
    "        url_pattern = re.compile(\"https:\\/\\/[\\w./]*[\\w/]\")\n",
    "        text = re.sub(at_pattern,'',text)\n",
    "        text = re.sub(url_pattern,'',text)\n",
    "        tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "        tokens = tokenizer.tokenize(text.lower())\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        stop_words.update(list(string.punctuation))\n",
    "        filtered = [w for w in tokens if w not in stop_words]\n",
    "        filtered = [w for w in tokens if w.isalpha()]\n",
    "        lemmatizaer = WordNetLemmatizer()\n",
    "        for word, tag in nltk.pos_tag(filtered):\n",
    "            if tag.startswith('NN'):\n",
    "                word= lemmatizaer.lemmatize(word, pos='n')\n",
    "            elif tag.startswith('VB'):\n",
    "                word= lemmatizaer.lemmatize(word, pos='v')\n",
    "            elif tag.startswith('JJ'):\n",
    "                word= lemmatizaer.lemmatize(word, pos='a')\n",
    "            elif tag.startswith('R'):\n",
    "                word= lemmatizaer.lemmatize(word, pos='r')\n",
    "            res.append(word)\n",
    "    return res\n",
    "\n",
    "def boolean_query(q):\n",
    "    for word in q:\n",
    "        if(word in invert_dict):\n",
    "            index=set(invert_dict[word])\n",
    "            break\n",
    "    for word in q:\n",
    "        if(len(word)>1 and word in invert_dict):\n",
    "            index=index&set(invert_dict[word])\n",
    "    index=list(index)\n",
    "    index.sort()\n",
    "    return index\n",
    "\n",
    "# generate tf-idf vector of query\n",
    "def gen_vector(tokens):\n",
    "    Q = np.zeros((len(vocabulary)))   \n",
    "    for token in tokens:\n",
    "        try:\n",
    "            ind = tfidf.vocabulary_[token]\n",
    "            Q[ind]  += 1/len(tokens)\n",
    "        except:\n",
    "            pass     \n",
    "    for token in tokens:\n",
    "        try:\n",
    "            ind = tfidf.vocabulary_[token]\n",
    "            Q[ind]  = tfidf.idf_[ind]\n",
    "        except:\n",
    "            pass\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35fdfaef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_rank(query,k):\n",
    "    q = text_process(query)\n",
    "    query_vector = gen_vector(q)\n",
    "    #boolean query\n",
    "    index = boolean_query(q) \n",
    "    #rank the documents in index\n",
    "    d_cosines = []\n",
    "    for doc in index:\n",
    "        #get the tfidf of doc\n",
    "        d = np.asarray(tfidf_tran.getrow(id_index[int(doc)]).todense())\n",
    "        #compute cosine similarity\n",
    "        cos_sim = cosine_similarity(query_vector.reshape(1, -1), d.reshape(1, -1))[0][0]\n",
    "        d_cosines.append(cos_sim)   \n",
    "    out = np.array(d_cosines).argsort()[-k:][::-1]\n",
    "\n",
    "    res = []\n",
    "    for i in range(k):\n",
    "        res.append(int(index[out[i]]))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "65e7e8f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res = search_rank(\"CHINA BEIJING CORONAVIRUS\",10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27f5a6c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1590601401385697280, 1590494088255741953, 1588142773349371906, 1588172706180386816, 1590216955704885249, 1588412085695815680, 1588412082583736320, 1587956423287508992, 1589406732773920768, 1590131480541687808]\n"
     ]
    }
   ],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "19aa78f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>created_at</th>\n",
       "      <th>full_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1587956423287508992</td>\n",
       "      <td>Wed Nov 02 23:54:48 +0000 2022</td>\n",
       "      <td>CHINA'S BEIJING REPORTS 28 NEW LOCAL SYMPTOMAT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28272</th>\n",
       "      <td>1588142773349371906</td>\n",
       "      <td>Thu Nov 03 12:15:18 +0000 2022</td>\n",
       "      <td>China Covid: Universal Resort shuts due to Bei...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34256</th>\n",
       "      <td>1588172706180386816</td>\n",
       "      <td>Thu Nov 03 14:14:14 +0000 2022</td>\n",
       "      <td>China Coronavirus Live: Why Beijing Locked Dow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70779</th>\n",
       "      <td>1588412085695815680</td>\n",
       "      <td>Fri Nov 04 06:05:27 +0000 2022</td>\n",
       "      <td>Germany's leader and top CEOs have arrived in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70780</th>\n",
       "      <td>1588412082583736320</td>\n",
       "      <td>Fri Nov 04 06:05:26 +0000 2022</td>\n",
       "      <td>Germany's leader and top CEOs have arrived in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212001</th>\n",
       "      <td>1589406732773920768</td>\n",
       "      <td>Sun Nov 06 23:57:49 +0000 2022</td>\n",
       "      <td>CHINA'S BEIJING REPORTS 41 NEW LOCAL SYMPTOMAT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333850</th>\n",
       "      <td>1590131480541687808</td>\n",
       "      <td>Tue Nov 08 23:57:42 +0000 2022</td>\n",
       "      <td>CHINA'S BEIJING REPORTS 32 NEW LOCAL SYMPTOMAT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344726</th>\n",
       "      <td>1590216955704885249</td>\n",
       "      <td>Wed Nov 09 05:37:21 +0000 2022</td>\n",
       "      <td>China Coronavirus Live: Why Beijing Locked Dow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384439</th>\n",
       "      <td>1590494088255741953</td>\n",
       "      <td>Wed Nov 09 23:58:35 +0000 2022</td>\n",
       "      <td>Beijing reports 95 new coronavirus cases, low ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398115</th>\n",
       "      <td>1590601401385697280</td>\n",
       "      <td>Thu Nov 10 07:05:00 +0000 2022</td>\n",
       "      <td>Beijing reports 95 new coronavirus cases, low ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         ID                      created_at  \\\n",
       "0       1587956423287508992  Wed Nov 02 23:54:48 +0000 2022   \n",
       "28272   1588142773349371906  Thu Nov 03 12:15:18 +0000 2022   \n",
       "34256   1588172706180386816  Thu Nov 03 14:14:14 +0000 2022   \n",
       "70779   1588412085695815680  Fri Nov 04 06:05:27 +0000 2022   \n",
       "70780   1588412082583736320  Fri Nov 04 06:05:26 +0000 2022   \n",
       "212001  1589406732773920768  Sun Nov 06 23:57:49 +0000 2022   \n",
       "333850  1590131480541687808  Tue Nov 08 23:57:42 +0000 2022   \n",
       "344726  1590216955704885249  Wed Nov 09 05:37:21 +0000 2022   \n",
       "384439  1590494088255741953  Wed Nov 09 23:58:35 +0000 2022   \n",
       "398115  1590601401385697280  Thu Nov 10 07:05:00 +0000 2022   \n",
       "\n",
       "                                                full_text  \n",
       "0       CHINA'S BEIJING REPORTS 28 NEW LOCAL SYMPTOMAT...  \n",
       "28272   China Covid: Universal Resort shuts due to Bei...  \n",
       "34256   China Coronavirus Live: Why Beijing Locked Dow...  \n",
       "70779   Germany's leader and top CEOs have arrived in ...  \n",
       "70780   Germany's leader and top CEOs have arrived in ...  \n",
       "212001  CHINA'S BEIJING REPORTS 41 NEW LOCAL SYMPTOMAT...  \n",
       "333850  CHINA'S BEIJING REPORTS 32 NEW LOCAL SYMPTOMAT...  \n",
       "344726  China Coronavirus Live: Why Beijing Locked Dow...  \n",
       "384439  Beijing reports 95 new coronavirus cases, low ...  \n",
       "398115  Beijing reports 95 new coronavirus cases, low ...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_text[full_text['ID'].isin(res)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd9da6e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         ID                      created_at  \\\n",
      "0       1587956423287508992  Wed Nov 02 23:54:48 +0000 2022   \n",
      "211995  1589406754231779330  Sun Nov 06 23:57:54 +0000 2022   \n",
      "212001  1589406732773920768  Sun Nov 06 23:57:49 +0000 2022   \n",
      "212076  1589407497634422785  Mon Nov 07 00:00:51 +0000 2022   \n",
      "333846  1590131506797719553  Tue Nov 08 23:57:49 +0000 2022   \n",
      "333847  1590131502179766272  Tue Nov 08 23:57:47 +0000 2022   \n",
      "333850  1590131480541687808  Tue Nov 08 23:57:42 +0000 2022   \n",
      "384221  1590492846137626625  Wed Nov 09 23:53:39 +0000 2022   \n",
      "384222  1590492844615155715  Wed Nov 09 23:53:38 +0000 2022   \n",
      "384224  1590492823107010560  Wed Nov 09 23:53:33 +0000 2022   \n",
      "\n",
      "                                                full_text  \n",
      "0       CHINA'S BEIJING REPORTS 28 NEW LOCAL SYMPTOMAT...  \n",
      "211995  CHINA'S BEIJING REPORTS 41 NEW LOCAL SYMPTOMAT...  \n",
      "212001  CHINA'S BEIJING REPORTS 41 NEW LOCAL SYMPTOMAT...  \n",
      "212076  CHINA'S BEIJING REPORTS 41 NEW LOCAL SYMPTOMAT...  \n",
      "333846  CHINA'S BEIJING REPORTS 32 NEW LOCAL SYMPTOMAT...  \n",
      "333847  CHINA'S BEIJING REPORTS 32 NEW LOCAL SYMPTOMAT...  \n",
      "333850  CHINA'S BEIJING REPORTS 32 NEW LOCAL SYMPTOMAT...  \n",
      "384221  CHINA'S BEIJING REPORTS 34 NEW LOCAL SYMPTOMAT...  \n",
      "384222  CHINA'S BEIJING REPORTS 34 NEW LOCAL SYMPTOMAT...  \n",
      "384224  CHINA'S BEIJING REPORTS 34 NEW LOCAL SYMPTOMAT...  \n"
     ]
    }
   ],
   "source": [
    "print(full_text[full_text['ID'].isin(res)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64db4ba6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
